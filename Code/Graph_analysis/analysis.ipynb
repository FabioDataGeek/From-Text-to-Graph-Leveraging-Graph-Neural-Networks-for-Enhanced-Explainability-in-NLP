{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/usrvol/processed_data/SNLI/test/constituency/constituency0.pkl\", \"rb\") as f:\n",
    "    graphs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics with networkX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = len(graphs[0][0].nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_edges = len(graphs[0][0].edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(graphs[0][0].degree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_degree = sum(degree_dict.values()) / len(degree_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness = nx.betweenness_centrality(G[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': 0.0,\n",
       " 'NP0': 0.003694581280788177,\n",
       " 0: 0.0,\n",
       " 5: 0.0,\n",
       " 12: 0.0,\n",
       " 'VP1': 0.02832512315270936,\n",
       " 18: 0.0,\n",
       " 'PP11': 0.009852216748768473,\n",
       " 24: 0.0,\n",
       " 'NP111': 0.007389162561576354,\n",
       " 27: 0.0,\n",
       " 31: 0.0,\n",
       " 'SBAR12': 0.04064039408866995,\n",
       " 32: 0.0,\n",
       " 'S121': 0.05665024630541872,\n",
       " 'NP1210': 0.0049261083743842365,\n",
       " 41: 0.0,\n",
       " 'VP1211': 0.06403940886699508,\n",
       " 'NP12111': 0.046798029556650245,\n",
       " 'NP121110': 0.014778325123152709,\n",
       " 51: 0.0,\n",
       " 58: 0.0,\n",
       " 'PP121111': 0.027093596059113302,\n",
       " 64: 0.0,\n",
       " 'NP1211111': 0.014778325123152709,\n",
       " 73: 0.0,\n",
       " 'PP12112': 0.019704433497536946,\n",
       " 78: 0.0,\n",
       " 'NP121121': 0.009852216748768473,\n",
       " 89: 0.0}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betweenness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Clustering Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_clustering_coeff = nx.clustering(G[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_local_clustering_coeff = sum(local_clustering_coeff.values()) / len(local_clustering_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Clustering Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_clustering_coeff = nx.transitivity(G[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = nx.density(G[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = nx.pagerank(G[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics with Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arguments import *\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset_GNN(root=args['root_test_data_path'], files_path=args['raw_test_data_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = len(dataset[0][0].x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_edges = len(dataset[0][0].edge_index1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.utils as tg_utils\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def calculate_average_degree(data):\n",
    "    \"\"\"\n",
    "    Calculate the average degree of a PyTorch Geometric graph.\n",
    "\n",
    "    Parameters:\n",
    "    - data (torch_geometric.data.Data): The graph data object containing edge_index.\n",
    "\n",
    "    Returns:\n",
    "    - average_degree (float): The average degree of all nodes in the graph.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric graph to NetworkX graph\n",
    "    G = tg_utils.to_networkx(data, to_undirected=True, remove_self_loops=True)\n",
    "    # Use NetworkX to calculate the degrees of all nodes\n",
    "    degrees = dict(G.degree())\n",
    "    # Calculate the average degree\n",
    "    average_degree = sum(degrees.values()) / len(degrees)\n",
    "    return average_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Degree for data1 (edge_index1): 1.9\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average degree for the first graph representation (undirected)\n",
    "average_degree1 = calculate_average_degree(Data(edge_index=dataset[0][0].edge_index1, num_nodes=dataset[0][0].x1.size(0)))\n",
    "print(f\"Average Degree for data1 (edge_index1): {average_degree1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweenness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_betweenness_centrality(data):\n",
    "    \"\"\"\n",
    "    Calculate the average betweenness centrality of a PyTorch Geometric graph.\n",
    "\n",
    "    Parameters:\n",
    "    - data (torch_geometric.data.Data): The graph data object containing edge_index.\n",
    "\n",
    "    Returns:\n",
    "    - average_betweenness_centrality (float): The average betweenness centrality score of all nodes.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric graph to NetworkX graph\n",
    "    G = tg_utils.to_networkx(data, to_undirected=True, remove_self_loops=True)\n",
    "    # Use NetworkX to calculate Betweenness Centrality\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    # Calculate the average betweenness centrality\n",
    "    average_betweenness_centrality = sum(betweenness_centrality.values()) / len(betweenness_centrality)\n",
    "    return average_betweenness_centrality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Betweenness Centrality for data1 (edge_index1): 0.17076023391812867\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average Betweenness Centrality for the first graph representation (undirected)\n",
    "average_betweenness_centrality1 = calculate_betweenness_centrality(Data(edge_index=dataset[0][0].edge_index1, num_nodes=dataset[0][0].x1.size(0)))\n",
    "print(f\"Average Betweenness Centrality for data1 (edge_index1): {average_betweenness_centrality1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.utils as tg_utils\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def calculate_mean_clustering_coefficient(data):\n",
    "    \"\"\"\n",
    "    Calculate the mean clustering coefficient for a PyTorch Geometric graph.\n",
    "\n",
    "    Parameters:\n",
    "    - data (torch_geometric.data.Data): The graph data object containing edge_index.\n",
    "\n",
    "    Returns:\n",
    "    - mean_clustering_coefficient (float): The mean clustering coefficient for the graph.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric graph to NetworkX graph\n",
    "    G = tg_utils.to_networkx(data, to_undirected=True, remove_self_loops=True)\n",
    "    # Use NetworkX to calculate the local clustering coefficient\n",
    "    clustering_coefficients = nx.clustering(G)\n",
    "    # Calculate and return the mean clustering coefficient\n",
    "    mean_clustering_coefficient = sum(clustering_coefficients.values()) / len(clustering_coefficients)\n",
    "    return mean_clustering_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "data = Data(edge_index=dataset[0][3].edge_index2, num_nodes=len(dataset[0][0].x2))  # Simple example graph\n",
    "mean_clustering_coefficient = calculate_mean_clustering_coefficient(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_clustering_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Clustering Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.utils as tg_utils\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_global_clustering_coefficient(data):\n",
    "    \"\"\"\n",
    "    Calculate the global clustering coefficient for a PyTorch Geometric graph.\n",
    "\n",
    "    Parameters:\n",
    "    - data (torch_geometric.data.Data): The graph data object containing edge_index.\n",
    "\n",
    "    Returns:\n",
    "    - global_clustering_coefficient (float): The global clustering coefficient for the graph.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric graph to NetworkX graph\n",
    "    G = tg_utils.to_networkx(data, to_undirected=True, remove_self_loops=True)\n",
    "    # Use NetworkX to calculate the global clustering coefficient\n",
    "    global_clustering_coefficient = nx.transitivity(G)\n",
    "    return global_clustering_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_clustering_coefficient1 = calculate_global_clustering_coefficient(Data(edge_index=dataset[0][0].edge_index1, num_nodes=dataset[0][0].x1.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_clustering_coefficient1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_density(data):\n",
    "    \"\"\"\n",
    "    Calculate the density of a PyTorch Geometric graph.\n",
    "\n",
    "    Parameters:\n",
    "    - data (torch_geometric.data.Data): The graph data object containing edge_index.\n",
    "\n",
    "    Returns:\n",
    "    - density (float): The density of the graph.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric graph to NetworkX graph\n",
    "    G = tg_utils.to_networkx(data, to_undirected=True, remove_self_loops=True)\n",
    "    # Use NetworkX to calculate the density of the graph\n",
    "    density = nx.density(G)\n",
    "    return density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density for data1 (edge_index1): 0.1\n"
     ]
    }
   ],
   "source": [
    "density1 = calculate_density(Data(edge_index=dataset[0][0].edge_index1, num_nodes=dataset[0][0].x1.size(0)))\n",
    "print(f\"Density for data1 (edge_index1): {density1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the graph has a relatively uniform distribution of PageRank scores, the average will be similar across different nodes. If some nodes have significantly high centrality, the average PageRank will be higher. This is a good measure if you want to compare different graphs or understand the spread of importance across nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.utils as tg_utils\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def calculate_average_pagerank(data, alpha=0.85):\n",
    "    \"\"\"\n",
    "    Calculate the PageRank of each node in a PyTorch Geometric graph.\n",
    "\n",
    "    Parameters:\n",
    "    - data (torch_geometric.data.Data): The graph data object containing edge_index.\n",
    "    - alpha (float): The damping factor for PageRank, default is 0.85.\n",
    "\n",
    "    Returns:\n",
    "    - pagerank (dict): A dictionary containing the PageRank score for each node.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric graph to NetworkX graph\n",
    "    G = tg_utils.to_networkx(data, to_undirected=True, remove_self_loops=True)\n",
    "    # Use NetworkX to calculate PageRank\n",
    "    pagerank = nx.pagerank(G, alpha=alpha)\n",
    "    average_pagerank = sum(pagerank.values()) / len(pagerank)\n",
    "    return average_pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank for data1 (edge_index1): 0.05000000000000001\n"
     ]
    }
   ],
   "source": [
    "# Calculate the PageRank for the first graph representation (undirected)\n",
    "pagerank = calculate_average_pagerank(Data(edge_index=dataset[0][0].edge_index1, num_nodes=dataset[0][0].x1.size(0)))\n",
    "print(f\"PageRank for data1 (edge_index1): {pagerank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total value itself may not provide significant new information, but it can be useful when comparing how the centrality is distributed or when using different damping factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.utils as tg_utils\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def calculate_total_pagerank(data, alpha=0.85):\n",
    "    \"\"\"\n",
    "    Calculate the PageRank of each node in a PyTorch Geometric graph.\n",
    "\n",
    "    Parameters:\n",
    "    - data (torch_geometric.data.Data): The graph data object containing edge_index.\n",
    "    - alpha (float): The damping factor for PageRank, default is 0.85.\n",
    "\n",
    "    Returns:\n",
    "    - pagerank (dict): A dictionary containing the PageRank score for each node.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric graph to NetworkX graph\n",
    "    G = tg_utils.to_networkx(data, to_undirected=True, remove_self_loops=True)\n",
    "    # Use NetworkX to calculate PageRank\n",
    "    pagerank = nx.pagerank(G, alpha=alpha)\n",
    "    total_pagerank = sum(pagerank.values())\n",
    "    return total_pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank for data1 (edge_index1): 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Calculate the PageRank for the first graph representation (undirected)\n",
    "pagerank = calculate_total_pagerank(Data(edge_index=dataset[0][0].edge_index2, num_nodes=dataset[0][0].x2.size(0)))\n",
    "print(f\"PageRank for data1 (edge_index1): {pagerank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high standard deviation means that some nodes have much higher importance than others, indicating a more hierarchical or unequal graph structure. A low standard deviation implies that importance is more evenly spread across nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.utils as tg_utils\n",
    "import networkx as nx\n",
    "import statistics\n",
    "\n",
    "def calculate_std_dev_pagerank(data, alpha=0.85):\n",
    "    \"\"\"\n",
    "    Calculate the PageRank of each node in a PyTorch Geometric graph.\n",
    "\n",
    "    Parameters:\n",
    "    - data (torch_geometric.data.Data): The graph data object containing edge_index.\n",
    "    - alpha (float): The damping factor for PageRank, default is 0.85.\n",
    "\n",
    "    Returns:\n",
    "    - pagerank (dict): A dictionary containing the PageRank score for each node.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric graph to NetworkX graph\n",
    "    G = tg_utils.to_networkx(data, to_undirected=True, remove_self_loops=True)\n",
    "    # Use NetworkX to calculate PageRank\n",
    "    pagerank = nx.pagerank(G, alpha=alpha)\n",
    "    pagerank_scores = list(pagerank.values())\n",
    "    pagerank_std_dev = statistics.stdev(pagerank_scores)\n",
    "    return pagerank_std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank standard deviation for data1 (edge_index1): 0.05096809409151725\n"
     ]
    }
   ],
   "source": [
    "# Calculate the PageRank for the first graph representation (undirected)\n",
    "pagerank = calculate_std_dev_pagerank(Data(edge_index=dataset[0][1].edge_index1, num_nodes=dataset[0][1].x1.size(0)))\n",
    "print(f\"PageRank standard deviation for data1 (edge_index1): {pagerank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher entropy means that PageRank is distributed more uniformly across nodes, whereas lower entropy indicates that a few nodes dominate the importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.utils as tg_utils\n",
    "import networkx as nx\n",
    "import math\n",
    "\n",
    "def calculate_entropy_pagerank(data, alpha=0.85):\n",
    "    \"\"\"\n",
    "    Calculate the PageRank of each node in a PyTorch Geometric graph.\n",
    "\n",
    "    Parameters:\n",
    "    - data (torch_geometric.data.Data): The graph data object containing edge_index.\n",
    "    - alpha (float): The damping factor for PageRank, default is 0.85.\n",
    "\n",
    "    Returns:\n",
    "    - pagerank (dict): A dictionary containing the PageRank score for each node.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric graph to NetworkX graph\n",
    "    G = tg_utils.to_networkx(data, to_undirected=True, remove_self_loops=True)\n",
    "    # Use NetworkX to calculate PageRank\n",
    "    pagerank = nx.pagerank(G, alpha=alpha)\n",
    "    pagerank_entropy = -sum(p * math.log(p) for p in pagerank.values() if p > 0)\n",
    "    return pagerank_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank entropy for data1 (edge_index1): 2.450868311619199\n"
     ]
    }
   ],
   "source": [
    "# Calculate the PageRank for the first graph representation (undirected)\n",
    "pagerank = calculate_entropy_pagerank(Data(edge_index=dataset[0][1].edge_index1, num_nodes=dataset[0][1].x1.size(0)))\n",
    "print(f\"PageRank entropy for data1 (edge_index1): {pagerank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric identifies which node in the graph holds the most influence or importance. This is useful for quickly finding the most influential node and understanding the relative dominance within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.utils as tg_utils\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_maximum_pagerank(data, alpha=0.85):\n",
    "    \"\"\"\n",
    "    Calculate the PageRank of each node in a PyTorch Geometric graph.\n",
    "\n",
    "    Parameters:\n",
    "    - data (torch_geometric.data.Data): The graph data object containing edge_index.\n",
    "    - alpha (float): The damping factor for PageRank, default is 0.85.\n",
    "\n",
    "    Returns:\n",
    "    - pagerank (dict): A dictionary containing the PageRank score for each node.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric graph to NetworkX graph\n",
    "    G = tg_utils.to_networkx(data, to_undirected=True, remove_self_loops=True)\n",
    "    # Use NetworkX to calculate PageRank\n",
    "    pagerank = nx.pagerank(G, alpha=alpha)\n",
    "    max_pagerank = max(pagerank.values())\n",
    "    return max_pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum PageRank for data1 (edge_index1): 0.12776438625366332\n"
     ]
    }
   ],
   "source": [
    "# Calculate the PageRank for the first graph representation (undirected)\n",
    "pagerank = calculate_maximum_pagerank(Data(edge_index=dataset[0][0].edge_index1, num_nodes=dataset[0][0].x1.size(0)))\n",
    "print(f\"Maximum PageRank for data1 (edge_index1): {pagerank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General PageRank Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.utils as tg_utils\n",
    "import networkx as nx\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "\n",
    "def calculate_pagerank(data, alpha=0.85):\n",
    "    \"\"\"\n",
    "    Calculate the PageRank of each node in a PyTorch Geometric graph.\n",
    "\n",
    "    Parameters:\n",
    "    - data (torch_geometric.data.Data): The graph data object containing edge_index.\n",
    "    - alpha (float): The damping factor for PageRank, default is 0.85.\n",
    "\n",
    "    Returns:\n",
    "    - pagerank (dict): A dictionary containing the PageRank score for each node.\n",
    "    \"\"\"\n",
    "    # Convert PyTorch Geometric graph to NetworkX graph\n",
    "    G = tg_utils.to_networkx(data, to_undirected=True, remove_self_loops=True)\n",
    "    # Use NetworkX to calculate PageRank\n",
    "    pagerank = nx.pagerank(G, alpha=alpha)\n",
    "    average_pagerank = sum(pagerank.values()) / len(pagerank)\n",
    "    total_pagerank = sum(pagerank.values())\n",
    "    pagerank_scores = list(pagerank.values())\n",
    "    pagerank_std_dev = statistics.stdev(pagerank_scores)\n",
    "    pagerank_entropy = -sum(p * math.log(p) for p in pagerank.values() if p > 0)\n",
    "    max_pagerank = max(pagerank.values())\n",
    "\n",
    "    results = {\n",
    "        'average_pagerank': average_pagerank,\n",
    "        'total_pagerank': total_pagerank,\n",
    "        'pagerank_std_dev': pagerank_std_dev,\n",
    "        'pagerank_entropy': pagerank_entropy,\n",
    "        'max_pagerank': max_pagerank\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum PageRank for data1 (edge_index1): 0.10623133512032584\n",
      "Average PageRank for data1 (edge_index1): 0.06666666666666665\n",
      "Total PageRank for data1 (edge_index1): 0.9999999999999998\n",
      "PageRank standard deviation for data1 (edge_index1): 0.027278800047661263\n",
      "PageRank entropy for data1 (edge_index1): 2.6282297599056035\n"
     ]
    }
   ],
   "source": [
    "# Calculate the PageRank for the first graph representation (undirected)\n",
    "pagerank = calculate_pagerank(Data(edge_index=dataset[0][15].edge_index1, num_nodes=dataset[0][15].x1.size(0)))\n",
    "print(f\"Maximum PageRank for data1 (edge_index1): {pagerank['max_pagerank']}\")\n",
    "print(f\"Average PageRank for data1 (edge_index1): {pagerank['average_pagerank']}\")\n",
    "print(f\"Total PageRank for data1 (edge_index1): {pagerank['total_pagerank']}\")\n",
    "print(f\"PageRank standard deviation for data1 (edge_index1): {pagerank['pagerank_std_dev']}\")\n",
    "print(f\"PageRank entropy for data1 (edge_index1): {pagerank['pagerank_entropy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Degree Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta función obtenemos diferentes métricas relacionadas con el grado del grafo, como es el grado promedio, grado máximo y mínimo, distribución del grado y porcentaje de nodos con bajo grado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculo automático de rollout y expand_Atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer que los parámetros **`rollout`** y **`expand_atoms`** dependan del **tamaño total del grafo** (es decir, el número de nodos y aristas) es una estrategia lógica y efectiva. Los grafos con distintos tamaños y estructuras tienen requerimientos de búsqueda diferentes, y adaptar estos parámetros a las características del grafo puede conducir a una mejor explicación de la estructura y comportamiento del grafo. A continuación, detallo cómo se podría establecer esta dependencia y cómo influiría en los valores óptimos para `rollout` y `expand_atoms`:\n",
    "\n",
    "### Dependencia del Tamaño Total del Grafo\n",
    "\n",
    "#### 1. **`rollout` Dependiente del Tamaño del Grafo**\n",
    "El parámetro **`rollout`** representa el **número de simulaciones** o el **número de iteraciones** que se realizan durante la búsqueda con **Monte Carlo Tree Search (MCTS)** para explorar posibles expansiones del grafo. Su relación con el tamaño total del grafo es importante:\n",
    "\n",
    "- **Grafo Pequeño**: \n",
    "  - Un grafo pequeño tiene menos nodos y aristas, lo que significa que hay un **espacio de búsqueda limitado**. En este caso, un `rollout` pequeño sería suficiente para explorar exhaustivamente la estructura del grafo. Un `rollout` demasiado alto resultaría en simulaciones redundantes, aumentando el costo computacional sin beneficio adicional.\n",
    "  \n",
    "- **Grafo Grande**: \n",
    "  - Un grafo grande tiene muchas más posibilidades de conexión y, por ende, el **espacio de búsqueda es mucho más amplio**. En este caso, se requiere un `rollout` mayor para tener una cobertura adecuada y explorar distintas partes del grafo. Un `rollout` bajo podría dejar sin explorar regiones potencialmente importantes, resultando en una explicación menos precisa.\n",
    "\n",
    "Entonces, podrías definir `rollout` como una **función del número de nodos (N)**. Una fórmula posible sería:\n",
    "\n",
    "\\[\n",
    "\\text{rollout} = \\alpha \\times N\n",
    "\\]\n",
    "\n",
    "donde **\\(\\alpha\\)** es un factor de ajuste que puedes elegir según el problema específico.\n",
    "\n",
    "Por ejemplo:\n",
    "- Para grafos con menos de 100 nodos, podrías elegir \\(\\alpha = 1\\), resultando en un `rollout` igual al número de nodos.\n",
    "- Para grafos más grandes, podrías incrementar \\(\\alpha\\), por ejemplo, \\(\\alpha = 2\\) para un mayor nivel de cobertura.\n",
    "\n",
    "#### 2. **`expand_atoms` Dependiente del Tamaño del Grafo**\n",
    "El parámetro **`expand_atoms`** controla el **número de nodos a expandir** durante la construcción de la búsqueda de MCTS. Es decir, cuántos nodos adicionales considerar cuando se amplía la exploración de un subgrafo. Su dependencia del tamaño del grafo también es fundamental:\n",
    "\n",
    "- **Grafo Pequeño**: \n",
    "  - En un grafo pequeño, donde hay menos nodos, es probable que se quiera mantener una **exploración controlada**. Si se expande demasiados nodos a la vez, es posible que se termine explorando una gran parte del grafo innecesariamente. Por lo tanto, es recomendable tener un `expand_atoms` relativamente pequeño en grafos pequeños.\n",
    "  \n",
    "- **Grafo Grande**:\n",
    "  - En un grafo grande, hay muchas más opciones para la expansión, por lo cual es útil tener un `expand_atoms` más grande para **explorar más nodos en cada iteración**, ya que esto aumentará las posibilidades de cubrir las partes más importantes del grafo. Si `expand_atoms` es pequeño en un grafo grande, se corre el riesgo de avanzar de manera muy lenta y de no alcanzar partes significativas del grafo en un tiempo razonable.\n",
    "\n",
    "Una posible fórmula para `expand_atoms` podría ser:\n",
    "\n",
    "\\[\n",
    "\\text{expand\\_atoms} = \\beta \\times \\log(N)\n",
    "\\]\n",
    "\n",
    "donde **\\(\\beta\\)** es un factor de ajuste.\n",
    "\n",
    "- Usar una función logarítmica tiene sentido aquí porque:\n",
    "  - A medida que aumenta el número de nodos, **`expand_atoms` crece más lentamente**, lo que ayuda a controlar el crecimiento del costo computacional.\n",
    "  - Para grafos grandes, se permite una expansión más significativa, pero sin que crezca linealmente con el tamaño del grafo, lo cual podría volverse inmanejable.\n",
    "\n",
    "#### 3. **Adaptación Combinada**\n",
    "Para adaptar estos dos parámetros de manera efectiva, podrías definir una estrategia combinada que tenga en cuenta tanto el **número de nodos (\\(N\\))** como el **número de aristas (\\(E\\))**:\n",
    "\n",
    "- **`rollout`**: Podría ser proporcional al número total de nodos, ajustado por un factor que dependa de la conectividad del grafo. Si el grafo es **muy disperso**, aumentar el valor de `rollout` puede ser más importante para tener una buena cobertura.\n",
    "\n",
    "  \\[\n",
    "  \\text{rollout} = \\gamma \\times N \\times \\left(1 + \\frac{1}{\\text{densidad}}\\right)\n",
    "  \\]\n",
    "\n",
    "  Donde la **densidad** es \\(\\frac{2E}{N(N-1)}\\), y \\(\\gamma\\) es un factor de ajuste. De esta manera, si el grafo es muy disperso (densidad baja), `rollout` se incrementa proporcionalmente para compensar la falta de conectividad.\n",
    "\n",
    "- **`expand_atoms`**: Podría depender tanto del número de nodos como del **grado promedio**:\n",
    "\n",
    "  \\[\n",
    "  \\text{expand\\_atoms} = \\eta \\times \\log(N) + \\delta \\times \\text{grado promedio}\n",
    "  \\]\n",
    "\n",
    "  donde \\(\\eta\\) y \\(\\delta\\) son factores de ajuste. Esto hace que `expand_atoms` aumente con el tamaño del grafo, pero también tenga en cuenta la conectividad local (grado promedio).\n",
    "\n",
    "\n",
    "### Conclusión\n",
    "Hacer que **`rollout`** y **`expand_atoms`** dependan del **tamaño del grafo** y otras métricas como **densidad** y **grado promedio** es una buena práctica para adaptar automáticamente el proceso de expansión y búsqueda en MCTS. Estas adaptaciones permiten encontrar mejores subgrafos explicativos con una cobertura más adecuada del grafo, independientemente de si es pequeño, grande, disperso o denso. Esta estrategia no solo mejora la calidad de la explicación, sino que también optimiza los recursos computacionales, evitando cálculos innecesarios o insuficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_beta_auto(graph, beta_0=0.1):\n",
    "    \"\"\"\n",
    "    Calcula el factor beta automáticamente según las propiedades del grafo,\n",
    "    con normalización de las métricas para asegurar contribuciones equilibradas.\n",
    "\n",
    "    Parámetros:\n",
    "        graph: Un objeto grafo de NetworkX.\n",
    "        beta_0: Valor base de beta.\n",
    "\n",
    "    Retorna:\n",
    "        beta: Valor calculado de beta.\n",
    "    \"\"\"\n",
    "    N = graph.number_of_nodes()\n",
    "    E = graph.number_of_edges()\n",
    "\n",
    "    # Evitar divisiones por cero\n",
    "    if N <= 1:\n",
    "        return beta_0\n",
    "\n",
    "    # Calcular densidad\n",
    "    D = nx.density(graph)  # Rango [0, 1]\n",
    "\n",
    "    # Calcular grado promedio\n",
    "    avg_degree = (2 * E) / N\n",
    "\n",
    "    # Calcular desviación estándar del grado\n",
    "    degrees = [degree for node, degree in graph.degree()]\n",
    "    std_degree = np.std(degrees)\n",
    "\n",
    "    # Normalizar grado promedio y desviación estándar usando logaritmo\n",
    "    avg_degree_norm = np.log(avg_degree + 1) / np.log(N)\n",
    "    std_degree_norm = np.log(std_degree + 1) / np.log(N)\n",
    "\n",
    "    # Limitar los valores normalizados a [0, 1]\n",
    "    avg_degree_norm = min(avg_degree_norm, 1)\n",
    "    std_degree_norm = min(std_degree_norm, 1)\n",
    "\n",
    "    # Calcular beta automáticamente\n",
    "    beta = beta_0 * (1 + (D + avg_degree_norm + std_degree_norm) / 3)\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rollout(graph, alfa_method, gamma=1, beta_0=0.1):\n",
    "    \"\"\"\n",
    "    Calcula el número de rollouts automáticamente según las propiedades del grafo.\n",
    "\n",
    "    Parámetros:\n",
    "        graph: Un objeto grafo de NetworkX.\n",
    "        gamma: Factor de escala para alfa en rollout.\n",
    "        beta_0: Valor base de beta.\n",
    "\n",
    "    Retorna:\n",
    "        rollout: Número de rollouts calculado automáticamente.\n",
    "    \"\"\"\n",
    "    N = graph.number_of_nodes()\n",
    "\n",
    "    # Calcular alfa basado en N\n",
    "    if alfa_method == 'log':\n",
    "        alpha = gamma * math.log10(N)\n",
    "    \n",
    "    elif alfa_method == 'step':\n",
    "        if N <= 10:\n",
    "            alpha = 1\n",
    "        elif N > 10 and N <= 100:\n",
    "            alpha = int(1 + 0.015*(N-10))\n",
    "        elif N > 100:\n",
    "            alpha = int(2.35+0.005*(N-100))\n",
    "\n",
    "    # Calcular beta automáticamente con métricas normalizadas\n",
    "    beta_auto = calculate_beta_auto(graph, beta_0)\n",
    "\n",
    "    # Calcular rollout\n",
    "    rollout = int(alpha * N * (1 + beta_auto))\n",
    "\n",
    "    return rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_expand_atoms(graph, eta_0=2):\n",
    "    \"\"\"\n",
    "    Calcula el valor de expand_atoms automáticamente según las propiedades del grafo.\n",
    "\n",
    "    Parámetros:\n",
    "        graph: Un objeto grafo de NetworkX.\n",
    "        eta_0: Valor base de expand_atoms.\n",
    "\n",
    "    Retorna:\n",
    "        expand_atoms: Valor calculado de expand_atoms.\n",
    "    \"\"\"\n",
    "    N = graph.number_of_nodes()\n",
    "    E = graph.number_of_edges()\n",
    "\n",
    "    # Evitar divisiones por cero\n",
    "    if N <= 1:\n",
    "        return eta_0\n",
    "\n",
    "    # Calcular grado promedio\n",
    "    avg_degree = (2 * E) / N\n",
    "\n",
    "    # Calcular desviación estándar del grado\n",
    "    degrees = [degree for node, degree in graph.degree()]\n",
    "    std_degree = np.std(degrees)\n",
    "\n",
    "    # Normalizar grado promedio y desviación estándar\n",
    "    avg_degree_norm = np.log(avg_degree + 1) / np.log(N)\n",
    "    std_degree_norm = np.log(std_degree + 1) / np.log(N)\n",
    "\n",
    "    # Limitar los valores normalizados a [0, 1]\n",
    "    avg_degree_norm = min(avg_degree_norm, 1)\n",
    "    std_degree_norm = min(std_degree_norm, 1)\n",
    "\n",
    "    # Calcular densidad\n",
    "    D = nx.density(graph)  # Ya está en el rango [0, 1]\n",
    "\n",
    "    # Calcular expand_atoms\n",
    "    expand_atoms = eta_0 * (1 + (avg_degree_norm + D + std_degree_norm) / 3)\n",
    "\n",
    "    # Asegurar que expand_atoms sea al menos 1 y entero\n",
    "    expand_atoms = max(int(expand_atoms), 1)\n",
    "\n",
    "    return expand_atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo automático de c_puct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, entiendo ahora. **Estás preguntando cuál es el rango habitual o los valores típicos de `c_puct` utilizados en SubgraphX**, no los valores calculados a partir de nuestra fórmula propuesta. Mis disculpas por la confusión.\n",
    "\n",
    "### **Rango Habitual de `c_puct` en SubgraphX**\n",
    "\n",
    "En el contexto de **SubgraphX**, el parámetro **`c_puct`** se utiliza dentro del algoritmo de **Monte Carlo Tree Search (MCTS)** para controlar el equilibrio entre **exploración** y **explotación** durante la construcción del árbol de búsqueda. Este parámetro es crucial para el rendimiento del algoritmo, y su valor afecta significativamente la calidad y eficiencia de las explicaciones generadas.\n",
    "\n",
    "#### **Valores Típicos de `c_puct`**\n",
    "\n",
    "- **Valor Predeterminado**: En la implementación original de SubgraphX, el valor de `c_puct` suele establecerse en **10**.\n",
    "- **Rango Común**: Los valores de `c_puct` en SubgraphX suelen variar entre **1** y **10**.\n",
    "- **Elección Empírica**: El valor exacto se determina a menudo mediante experimentación, ajustándose según el conjunto de datos específico y la complejidad de los grafos analizados.\n",
    "\n",
    "#### **Consideraciones al Seleccionar `c_puct`**\n",
    "\n",
    "1. **Equilibrio entre Exploración y Explotación**:\n",
    "   - **Valores Más Altos (e.g., `c_puct` = 10)**:\n",
    "     - Promueven una mayor **exploración**.\n",
    "     - El algoritmo tiende a visitar nodos menos explorados, buscando nuevas posibilidades.\n",
    "     - Útil en grafos complejos o cuando se desea una cobertura más amplia del espacio de búsqueda.\n",
    "   - **Valores Más Bajos (e.g., `c_puct` = 1)**:\n",
    "     - Favorecen la **explotación**.\n",
    "     - El algoritmo se centra más en los nodos con altas evaluaciones actuales.\n",
    "     - Puede ser beneficioso cuando se tiene confianza en las estimaciones actuales o se desea una convergencia más rápida.\n",
    "\n",
    "2. **Influencia del Tamaño y Estructura del Grafo**:\n",
    "   - **Grafos Pequeños o Simples**:\n",
    "     - Un valor menor de `c_puct` puede ser suficiente, ya que el espacio de búsqueda es más manejable.\n",
    "   - **Grafos Grandes o Densos**:\n",
    "     - Valores más altos de `c_puct` ayudan a explorar mejor el espacio de búsqueda complejo.\n",
    "\n",
    "3. **Dependencia del Modelo y la Tarea**:\n",
    "   - **Modelos con Alta Incertidumbre**:\n",
    "     - Un `c_puct` mayor permite al algoritmo explorar más opciones, lo cual es útil si las predicciones del modelo son menos confiables.\n",
    "   - **Tareas Críticas**:\n",
    "     - En aplicaciones donde es crucial encontrar explicaciones precisas, puede ser preferible un `c_puct` más alto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_c_puct(graph, c0=10.0, delta=1.0):\n",
    "    N = graph.number_of_nodes()\n",
    "    E = graph.number_of_edges()\n",
    "\n",
    "    if N <= 1:\n",
    "        return c0\n",
    "\n",
    "    D = nx.density(graph)  # Densidad del grafo\n",
    "\n",
    "    # Calcular grado promedio\n",
    "    avg_degree = (2 * E) / N\n",
    "\n",
    "    # Calcular desviación estándar del grado\n",
    "    degrees = [degree for node, degree in graph.degree()]\n",
    "    std_degree = np.std(degrees)\n",
    "\n",
    "    # Normalizar grado promedio y desviación estándar usando logaritmo\n",
    "    avg_degree_norm = np.log(avg_degree + 1) / np.log(N)\n",
    "    std_degree_norm = np.log(std_degree + 1) / np.log(N)\n",
    "\n",
    "    # Limitar los valores normalizados a [0, 1]\n",
    "    avg_degree_norm = min(max(avg_degree_norm, 0), 1)\n",
    "    std_degree_norm = min(max(std_degree_norm, 0), 1)\n",
    "\n",
    "    # Calcular el promedio de las métricas normalizadas\n",
    "    metrics_mean = (D + avg_degree_norm + std_degree_norm) / 3\n",
    "\n",
    "    # Ajustar c_puct\n",
    "    c_puct = c0 * (1 + delta * (metrics_mean - 0.5))\n",
    "\n",
    "    # Asegurar que c_puct no sea menor que una fracción del valor base (por ejemplo, no menos del 50%)\n",
    "    c_puct = max(c_puct, c0 * 0.5)\n",
    "\n",
    "    return c_puct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo automático de sample_num y min_atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Ejemplos y Tabla de Valores**\n",
    "\n",
    "Calculemos **`sample_num`** para diferentes valores de **\\(N\\)**, usando **\\(K = 100\\)**.\n",
    "\n",
    "### **Cálculos Preliminares**\n",
    "\n",
    "- **scale_factor**:\n",
    "  \\[\n",
    "  \\text{scale\\_factor} = \\frac{N}{N + 100}\n",
    "  \\]\n",
    "\n",
    "- **Denominador**:\n",
    "  \\[\n",
    "  1 - \\text{scale\\_factor} = \\frac{100}{N + 100}\n",
    "  \\]\n",
    "\n",
    "### **Tabla de Valores**\n",
    "\n",
    "| \\(N\\) | \\(\\text{min\\_atoms}\\) | \\(\\text{scale\\_factor}\\) | \\(1 - \\text{scale\\_factor}\\) | \\(\\log(N + 1)\\) | \\(\\text{sample\\_num}\\)                           | \\(\\text{sample\\_num}\\) (entero) |\n",
    "|-------|------------------------|--------------------------|------------------------------|-----------------|--------------------------------------------------|---------------------------|\n",
    "| 1     | 1                      | \\(1/101 \\approx 0.0099\\) | \\(100/101 \\approx 0.9901\\)   | 0.6931          | \\(1 + \\frac{0.6931}{0.9901} \\approx 1.6999\\)     | 1                         |\n",
    "| 5     | 1                      | \\(5/105 \\approx 0.0476\\) | \\(100/105 \\approx 0.9524\\)   | 1.7918          | \\(1 + \\frac{1.7918}{0.9524} \\approx 2.8824\\)     | 2                         |\n",
    "| 10    | 1                      | \\(10/110 \\approx 0.0909\\)| \\(100/110 \\approx 0.9091\\)   | 2.3979          | \\(1 + \\frac{2.3979}{0.9091} \\approx 3.6387\\)     | 3                         |\n",
    "| 20    | 2                      | \\(20/120 \\approx 0.1667\\)| \\(100/120 \\approx 0.8333\\)   | 3.0445          | \\(2 + \\frac{3.0445}{0.8333} \\approx 5.6527\\)     | 5                         |\n",
    "| 50    | 5                      | \\(50/150 \\approx 0.3333\\)| \\(100/150 \\approx 0.6667\\)   | 3.9318          | \\(5 + \\frac{3.9318}{0.6667} \\approx 10.8977\\)    | 10                        |\n",
    "| 100   | 10                     | \\(100/200 = 0.5\\)        | \\(0.5\\)                      | 4.6151          | \\(10 + \\frac{4.6151}{0.5} = 10 + 9.2302 = 19.2302\\)| 19                        |\n",
    "| 200   | 20                     | \\(200/300 \\approx 0.6667\\)| \\(100/300 \\approx 0.3333\\)  | 5.3033          | \\(20 + \\frac{5.3033}{0.3333} \\approx 35.9098\\)   | 35                        |\n",
    "| 500   | 50                     | \\(500/600 \\approx 0.8333\\)| \\(100/600 \\approx 0.1667\\)  | 6.2166          | \\(50 + \\frac{6.2166}{0.1667} \\approx 87.2990\\)   | 87                        |\n",
    "| 1000  | 100                    | \\(1000/1100 \\approx 0.9091\\)| \\(100/1100 \\approx 0.0909\\)| 6.9088          | \\(100 + \\frac{6.9088}{0.0909} \\approx 176.9977\\) | 176                       |\n",
    "\n",
    "### **Observaciones**\n",
    "\n",
    "- **Para \\(N\\) pequeños**:\n",
    "\n",
    "  - **`scale_factor`** es cercano a **0**, y **\\(1 - \\text{scale\\_factor}\\)** es cercano a **1**.\n",
    "  - El denominador es grande, lo que reduce el impacto de \\(\\log(N + 1)\\) en **`sample_num`**.\n",
    "  - **`sample_num`** es pequeño y no excede **\\(N\\)**.\n",
    "\n",
    "- **Para \\(N\\) grandes**:\n",
    "\n",
    "  - **`scale_factor`** se acerca a **1**, y **\\(1 - \\text{scale\\_factor}\\)** es pequeño.\n",
    "  - El denominador disminuye, aumentando el impacto de \\(\\log(N + 1)\\) en **`sample_num`**.\n",
    "  - **`sample_num`** aumenta significativamente, pero podemos controlarlo estableciendo límites o ajustando **\\(K\\)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def calculate_min_atoms(N):\n",
    "    \"\"\"\n",
    "    Calcula min_atoms como el 10% de N, redondeado al número entero más cercano,\n",
    "    asegurando que sea al menos 1.\n",
    "    \"\"\"\n",
    "    min_atoms = max(1, int(round(0.1 * N)))\n",
    "    return min_atoms\n",
    "\n",
    "def calculate_scale_factor(N, K=100):\n",
    "    \"\"\"\n",
    "    Calcula scale_factor como N / (N + K).\n",
    "    \"\"\"\n",
    "    scale_factor = N / (N + K)\n",
    "    return scale_factor\n",
    "\n",
    "def calculate_sample_num(N, min_atoms, K=100):\n",
    "    \"\"\"\n",
    "    Calcula sample_num usando min_atoms y scale_factor dependiente de N.\n",
    "    \"\"\"\n",
    "    scale_factor = calculate_scale_factor(N, K)\n",
    "    denominator = 1 - scale_factor  # Siempre positivo y menor o igual a 1\n",
    "\n",
    "    sample_num = min_atoms + np.log(N + 1) / denominator\n",
    "    sample_num = int(min(N, max(sample_num, min_atoms)))  # Asegura que sample_num esté entre min_atoms y N\n",
    "    return sample_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo automático de local_radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Claro! Ahora desarrollaré una metodología para calcular automáticamente el **`local_radius`** basándonos en las propiedades estructurales del grafo. El **`local_radius`** es un parámetro crucial que determina el alcance de la exploración o influencia alrededor de un nodo en el grafo. Calcularlo automáticamente permite adaptar el algoritmo a las características específicas de cada grafo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Metodología para Calcular `local_radius` Automáticamente**\n",
    "\n",
    "#### **1. Introducción**\n",
    "\n",
    "El **`local_radius`** debe reflejar la estructura y propiedades del grafo. Algunas métricas clave que podemos utilizar son:\n",
    "\n",
    "- **Longitud media de los caminos más cortos** (`average_shortest_path_length`): Indica cuán conectado está el grafo.\n",
    "- **Diámetro del grafo** (`diameter`): La distancia máxima entre cualquier par de nodos.\n",
    "- **Coeficiente de clustering** (`clustering coefficient`): Mide la tendencia de los nodos a agruparse.\n",
    "- **Grado medio de los nodos** (`average_degree`): Promedio de conexiones por nodo.\n",
    "- **Excentricidad**: La máxima distancia desde un nodo a cualquier otro nodo.\n",
    "\n",
    "#### **2. Propuesta de Métodos**\n",
    "\n",
    "##### **a) Basado en la Longitud Media de los Caminos Más Cortos**\n",
    "\n",
    "\\[\n",
    "\\text{local\\_radius} = \\alpha \\times \\text{Longitud Media de los Caminos Más Cortos}\n",
    "\\]\n",
    "\n",
    "- **Ventajas**: Refleja la conectividad general del grafo.\n",
    "- **Consideraciones**: Puede ser costoso de calcular en grafos muy grandes.\n",
    "\n",
    "##### **b) Basado en el Diámetro del Grafo**\n",
    "\n",
    "\\[\n",
    "\\text{local\\_radius} = \\beta \\times \\text{Diámetro del Grafo}\n",
    "\\]\n",
    "\n",
    "- **Ventajas**: Considera la máxima extensión del grafo.\n",
    "- **Consideraciones**: Sensible a valores extremos o grafos con componentes desconectados.\n",
    "\n",
    "##### **c) Basado en el Grado Medio de los Nodos**\n",
    "\n",
    "\\[\n",
    "\\text{local\\_radius} = \\gamma \\times \\left( \\frac{1}{\\text{Grado Medio}} \\right)\n",
    "\\]\n",
    "\n",
    "- **Ventajas**: Fácil de calcular y refleja la densidad local.\n",
    "- **Consideraciones**: En grafos muy densos, puede resultar en valores pequeños de `local_radius`.\n",
    "\n",
    "##### **d) Combinación de Múltiples Métricas**\n",
    "\n",
    "\\[\n",
    "\\text{local\\_radius} = \\delta \\times \\left( \\frac{\\text{Longitud Media} + \\text{Diámetro}}{2} \\right)\n",
    "\\]\n",
    "\n",
    "- **Ventajas**: Equilibra la conectividad global y máxima del grafo.\n",
    "- **Consideraciones**: Requiere calcular múltiples métricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def calculate_local_radius(graph, scale_factor=1.0):\n",
    "    \"\"\"\n",
    "    Calcula local_radius basado en el diámetro del grafo y el grado medio.\n",
    "\n",
    "    Parámetros:\n",
    "        graph: Un objeto grafo de NetworkX.\n",
    "        scale_factor: Factor de escala para ajustar local_radius.\n",
    "\n",
    "    Retorna:\n",
    "        local_radius: Valor calculado de local_radius.\n",
    "    \"\"\"\n",
    "    N = graph.number_of_nodes()\n",
    "    M = graph.number_of_edges()\n",
    "\n",
    "    # Calcular el grado medio\n",
    "    average_degree = (2 * M) / N\n",
    "\n",
    "    # Calcular el diámetro del grafo\n",
    "    diameter = nx.diameter(graph)\n",
    "\n",
    "\n",
    "    # Calcular alfa basado en N\n",
    "    if scale_factor == 'log':\n",
    "        scale_factor = math.log10(N)\n",
    "    \n",
    "    elif scale_factor == 'step':\n",
    "        if N <= 10:\n",
    "            scale_factor = 1\n",
    "        elif N > 10 and N <= 100:\n",
    "            scale_factor = int(1 + 0.015*(N-10))\n",
    "        elif N > 100:\n",
    "            scale_factor = int(2.35+0.005*(N-100))\n",
    "\n",
    "    # Calcular local_radius\n",
    "    local_radius = scale_factor * (diameter / average_degree)\n",
    "\n",
    "    # Aseguramos que local_radius sea al menos 1\n",
    "    local_radius = max(1, local_radius)\n",
    "\n",
    "    return local_radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo automático de num_hops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el código de `SubgraphX`, el parámetro **`num_hops`** es opcional. Esto significa que, si no lo especificas, el algoritmo aún funcionará correctamente utilizando un valor predeterminado o calculado internamente.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. `num_hops` es Opcional y Tiene un Valor Predeterminado**\n",
    "\n",
    "En la definición de la clase `SubgraphX`, el parámetro `num_hops` se establece con un valor predeterminado de `None`:\n",
    "\n",
    "```python\n",
    "def __init__(self, model, num_classes: int, device, num_hops: Optional[int] = None, verbose: bool = False, ...)\n",
    "```\n",
    "\n",
    "Si no proporcionas un valor para `num_hops` al instanciar `SubgraphX`, el método `update_num_hops` se encarga de asignar un valor adecuado basado en tu modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Cómo se Calcula el Valor Predeterminado de `num_hops`**\n",
    "\n",
    "El método `update_num_hops` verifica si `num_hops` es `None`. Si es así, cuenta el número de capas de propagación de mensajes en tu modelo GNN y asigna ese número a `num_hops`:\n",
    "\n",
    "```python\n",
    "def update_num_hops(self, num_hops):\n",
    "    if num_hops is not None:\n",
    "        return num_hops\n",
    "\n",
    "    k = 0\n",
    "    for module in self.model.modules():\n",
    "        if isinstance(module, MessagePassing):\n",
    "            k += 1\n",
    "    return k\n",
    "```\n",
    "\n",
    "Esto asegura que el valor de `num_hops` sea consistente con la profundidad de tu modelo GNN, lo que es razonable ya que el alcance de las características que el modelo puede capturar está relacionado con su número de capas.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Implicaciones de No Especificar `num_hops`**\n",
    "\n",
    "- **Funcionamiento Correcto del Algoritmo**: El algoritmo está diseñado para funcionar sin que tengas que especificar `num_hops`. Utiliza un valor predeterminado que se ajusta a la arquitectura de tu modelo.\n",
    "\n",
    "- **Consistencia con el Modelo**: Al basar `num_hops` en el número de capas de propagación de mensajes, el subgrafo extraído abarca la misma cantidad de saltos que el modelo utiliza para propagar información.\n",
    "\n",
    "- **Flexibilidad**: Puedes optar por dejar que el algoritmo determine `num_hops` automáticamente o proporcionarlo manualmente si deseas un control más preciso sobre el alcance del subgrafo.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. ¿Es Necesario Especificar `num_hops`?**\n",
    "\n",
    "No es necesario especificar `num_hops` para que el algoritmo funcione. De hecho, en muchos casos, es preferible dejar que el algoritmo lo calcule automáticamente, ya que esto asegura que el subgrafo utilizado para la explicación esté alineado con la arquitectura de tu modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Recomendaciones**\n",
    "\n",
    "- **Dejar que el Algoritmo Calcule `num_hops`**: Si no tienes una razón específica para establecer `num_hops`, es recomendable permitir que el algoritmo lo calcule. Esto simplifica el proceso y reduce la posibilidad de inconsistencias.\n",
    "\n",
    "- **Especificar `num_hops` si Tienes Requisitos Especiales**: Si, por alguna razón, deseas que el subgrafo abarque más o menos saltos que las capas de tu modelo, puedes especificar `num_hops` manualmente.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Ejemplo Práctico**\n",
    "\n",
    "#### **Sin Especificar `num_hops`**\n",
    "\n",
    "```python\n",
    "# Instanciar SubgraphX sin especificar num_hops\n",
    "subgraphx = SubgraphX(model=model, num_classes=2, device=device)\n",
    "\n",
    "# Utilizar el método explain\n",
    "_, explanation_results, related_preds = subgraphx(x_list, edge_index_list, batch_list)\n",
    "```\n",
    "\n",
    "En este ejemplo, `num_hops` será calculado automáticamente basado en el número de capas de propagación de mensajes en `model`.\n",
    "\n",
    "#### **Especificando `num_hops` Manualmente**\n",
    "\n",
    "```python\n",
    "# Instanciar SubgraphX especificando num_hops\n",
    "subgraphx = SubgraphX(model=model, num_classes=2, device=device, num_hops=3)\n",
    "\n",
    "# Utilizar el método explain\n",
    "_, explanation_results, related_preds = subgraphx(x_list, edge_index_list, batch_list)\n",
    "```\n",
    "\n",
    "Aquí, hemos fijado `num_hops` en 3, independientemente de la arquitectura del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Conclusión**\n",
    "\n",
    "Puedes utilizar el algoritmo sin especificar `num_hops`, ya que es un parámetro opcional y el algoritmo está diseñado para calcular un valor adecuado automáticamente. Esto garantiza que el subgrafo extraído para la explicación sea coherente con la capacidad de tu modelo para propagar información a través de sus capas.\n",
    "\n",
    "---\n",
    "\n",
    "### **Información Adicional**\n",
    "\n",
    "- **Interacción con Otros Parámetros**: Aunque `num_hops` es opcional, otros parámetros como `local_radius` y `sample_num` siguen siendo importantes para controlar aspectos específicos del algoritmo.\n",
    "\n",
    "- **Flexibilidad y Personalización**: La posibilidad de dejar ciertos parámetros como opcionales te da flexibilidad para ajustar el algoritmo según tus necesidades sin tener que preocuparte por cada detalle.\n",
    "\n",
    "---\n",
    "\n",
    "Si tienes más preguntas o necesitas ayuda con otros aspectos del algoritmo, ¡no dudes en consultarme! Estoy aquí para ayudarte a aprovechar al máximo las capacidades de `SubgraphX`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función general parámetros de entrada a subgraphX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "def calculate_subgraphx_parameters(model, graph, device, num_classes, num_hops=None, verbose=False,\n",
    "                                   explain_graph=True, high2low=False, reward_method='mc_l_shapley',\n",
    "                                   subgraph_building_method='zero_filling', save_dir=None,\n",
    "                                   filename='example', vis=True):\n",
    "    \"\"\"\n",
    "    Calcula todos los parámetros de entrada necesarios para SubgraphX basándose en el grafo y el modelo.\n",
    "\n",
    "    Parámetros:\n",
    "        model: El modelo GNN utilizado.\n",
    "        graph: El grafo de NetworkX.\n",
    "        device: El dispositivo ('cpu' o 'cuda') en el que se ejecuta el modelo.\n",
    "        num_classes: Número de clases del problema.\n",
    "        num_hops: Número de saltos para extraer el subgrafo (opcional).\n",
    "        verbose: Si se desea mostrar información adicional durante la ejecución.\n",
    "        explain_graph: Si se está explicando una tarea de clasificación de grafos.\n",
    "        high2low: Ordenar los nodos de alto a bajo grado al expandir en MCTS.\n",
    "        reward_method: Método de recompensa para SubgraphX.\n",
    "        subgraph_building_method: Método para construir subgrafos ('zero_filling' o 'split').\n",
    "        save_dir: Directorio para guardar los resultados.\n",
    "        filename: Nombre del archivo para guardar los resultados.\n",
    "        vis: Si se desea visualizar los resultados.\n",
    "\n",
    "    Retorna:\n",
    "        Un diccionario con todos los parámetros calculados para SubgraphX.\n",
    "    \"\"\"\n",
    "    N = graph.number_of_nodes()\n",
    "    M = graph.number_of_edges()\n",
    "\n",
    "    # Calcular min_atoms\n",
    "    min_atoms = max(1, int(round(0.1 * N)))\n",
    "\n",
    "    # Calcular sample_num\n",
    "    # Usando scale_factor dependiente de N\n",
    "    K = 100  # Constante ajustable\n",
    "    scale_factor = N / (N + K)\n",
    "    denominator = 1 - scale_factor\n",
    "    sample_num = min_atoms + np.log(N + 1) / denominator\n",
    "    sample_num = int(min(N, max(sample_num, min_atoms)))\n",
    "\n",
    "    # Calcular local_radius\n",
    "    # Calcular grado medio\n",
    "    average_degree = (2 * M) / N\n",
    "\n",
    "    # Calcular diámetro del grafo\n",
    "    try:\n",
    "        diameter = nx.diameter(graph)\n",
    "    except nx.NetworkXError:\n",
    "        # Si el grafo no es conectado, usar el diámetro del componente más grande\n",
    "        largest_cc = max(nx.connected_components(graph), key=len)\n",
    "        subgraph = graph.subgraph(largest_cc)\n",
    "        diameter = nx.diameter(subgraph)\n",
    "\n",
    "    # Calcular scale_factor para local_radius\n",
    "    a = 0.1  # Parámetro ajustable\n",
    "    b = 0.5  # Parámetro ajustable\n",
    "    scale_factor_lr = a * np.log(N) + b\n",
    "    local_radius = scale_factor_lr * (diameter / average_degree)\n",
    "    local_radius = max(1, local_radius)\n",
    "\n",
    "    # Calcular c_puct\n",
    "    c0 = 10.0  # Valor dado\n",
    "    delta = 1.0  # Valor dado\n",
    "    c_puct = c0 * np.log((N + delta) / delta)\n",
    "\n",
    "    # Calcular expand_atoms\n",
    "    eta_0 = 2  # Valor dado\n",
    "    expand_atoms = int(eta_0 * np.log(N + 1))\n",
    "\n",
    "    # Calcular rollout\n",
    "    gamma = 0.01  # Valor dado\n",
    "    beta_0 = 0.1  # Valor dado\n",
    "    alpha = gamma * N\n",
    "    beta_auto = beta_0  # Puedes ajustar beta_auto si tienes una función específica\n",
    "    rollout = int(alpha * N * (1 + beta_auto))\n",
    "\n",
    "    # Calcular num_hops si no se proporciona\n",
    "    if num_hops is None:\n",
    "        num_hops = 0\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                num_hops += 1\n",
    "\n",
    "    # Preparar el diccionario de parámetros\n",
    "    parameters = {\n",
    "        'model': model,\n",
    "        'num_classes': num_classes,\n",
    "        'device': device,\n",
    "        'num_hops': num_hops,\n",
    "        'verbose': verbose,\n",
    "        'explain_graph': explain_graph,\n",
    "        'rollout': rollout,\n",
    "        'min_atoms': min_atoms,\n",
    "        'c_puct': c_puct,\n",
    "        'expand_atoms': expand_atoms,\n",
    "        'high2low': high2low,\n",
    "        'local_radius': local_radius,\n",
    "        'sample_num': sample_num,\n",
    "        'reward_method': reward_method,\n",
    "        'subgraph_building_method': subgraph_building_method,\n",
    "        'save_dir': save_dir,\n",
    "        'filename': filename,\n",
    "        'vis': vis\n",
    "    }\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptación para N grafos en paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "def calculate_subgraphx_parameters(model, graphs, device, num_classes, num_hops=None, verbose=False,\n",
    "                                   explain_graph=True, high2low=False, reward_method='mc_l_shapley',\n",
    "                                   subgraph_building_method='zero_filling', save_dir=None,\n",
    "                                   filename='example', vis=True):\n",
    "    \"\"\"\n",
    "    Calcula los parámetros de entrada necesarios para SubgraphX para múltiples grafos.\n",
    "\n",
    "    Parámetros:\n",
    "        model: El modelo GNN utilizado.\n",
    "        graphs: Lista de grafos de NetworkX.\n",
    "        device: El dispositivo ('cpu' o 'cuda') en el que se ejecuta el modelo.\n",
    "        num_classes: Número de clases del problema.\n",
    "        num_hops: Número de saltos para extraer el subgrafo (opcional).\n",
    "        verbose: Si se desea mostrar información adicional durante la ejecución.\n",
    "        explain_graph: Si se está explicando una tarea de clasificación de grafos.\n",
    "        high2low: Ordenar los nodos de alto a bajo grado al expandir en MCTS.\n",
    "        reward_method: Método de recompensa para SubgraphX.\n",
    "        subgraph_building_method: Método para construir subgrafos ('zero_filling' o 'split').\n",
    "        save_dir: Directorio para guardar los resultados.\n",
    "        filename: Nombre del archivo para guardar los resultados.\n",
    "        vis: Si se desea visualizar los resultados.\n",
    "\n",
    "    Retorna:\n",
    "        Una lista de diccionarios con los parámetros calculados para cada grafo.\n",
    "    \"\"\"\n",
    "    # Parámetros compartidos\n",
    "    if num_hops is None:\n",
    "        num_hops = 0\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, MessagePassing):\n",
    "                num_hops += 1\n",
    "\n",
    "    # Lista para almacenar los parámetros de cada grafo\n",
    "    parameters_list = []\n",
    "\n",
    "    # Constantes ajustables\n",
    "    K = 100  # Para sample_num\n",
    "    a = 0.1  # Para local_radius\n",
    "    b = 0.5  # Para local_radius\n",
    "    c0 = 10.0  # Para c_puct\n",
    "    delta = 1.0  # Para c_puct\n",
    "    eta_0 = 2  # Para expand_atoms\n",
    "    gamma = 0.01  # Para rollout\n",
    "    beta_0 = 0.1  # Para rollout\n",
    "\n",
    "    # Iterar sobre cada grafo y calcular los parámetros\n",
    "    for idx, graph in enumerate(graphs):\n",
    "        N = graph.number_of_nodes()\n",
    "        M = graph.number_of_edges()\n",
    "\n",
    "        # Calcular min_atoms\n",
    "        min_atoms = max(1, int(round(0.1 * N)))\n",
    "\n",
    "        # Calcular sample_num\n",
    "        scale_factor = N / (N + K)\n",
    "        denominator = 1 - scale_factor\n",
    "        sample_num = min_atoms + np.log(N + 1) / denominator\n",
    "        sample_num = int(min(N, max(sample_num, min_atoms)))\n",
    "\n",
    "        # Calcular local_radius\n",
    "        average_degree = (2 * M) / N\n",
    "\n",
    "        # Calcular diámetro del grafo\n",
    "        try:\n",
    "            diameter = nx.diameter(graph)\n",
    "        except nx.NetworkXError:\n",
    "            # Si el grafo no es conectado, usar el diámetro del componente más grande\n",
    "            largest_cc = max(nx.connected_components(graph), key=len)\n",
    "            subgraph = graph.subgraph(largest_cc)\n",
    "            diameter = nx.diameter(subgraph)\n",
    "\n",
    "        # Calcular scale_factor para local_radius\n",
    "        scale_factor_lr = a * np.log(N) + b\n",
    "        local_radius = scale_factor_lr * (diameter / average_degree)\n",
    "        local_radius = max(1, local_radius)\n",
    "\n",
    "        # Calcular c_puct\n",
    "        c_puct = c0 * np.log((N + delta) / delta)\n",
    "\n",
    "        # Calcular expand_atoms\n",
    "        expand_atoms = int(eta_0 * np.log(N + 1))\n",
    "\n",
    "        # Calcular rollout\n",
    "        alpha = gamma * N\n",
    "        beta_auto = beta_0  # Puedes ajustar beta_auto si tienes una función específica\n",
    "        rollout = int(alpha * N * (1 + beta_auto))\n",
    "\n",
    "        # Preparar el diccionario de parámetros para este grafo\n",
    "        parameters = {\n",
    "            'model': model,\n",
    "            'num_classes': num_classes,\n",
    "            'device': device,\n",
    "            'num_hops': num_hops,\n",
    "            'verbose': verbose,\n",
    "            'explain_graph': explain_graph,\n",
    "            'rollout': rollout,\n",
    "            'min_atoms': min_atoms,\n",
    "            'c_puct': c_puct,\n",
    "            'expand_atoms': expand_atoms,\n",
    "            'high2low': high2low,\n",
    "            'local_radius': local_radius,\n",
    "            'sample_num': sample_num,\n",
    "            'reward_method': reward_method,\n",
    "            'subgraph_building_method': subgraph_building_method,\n",
    "            'save_dir': save_dir,\n",
    "            'filename': f\"{filename}_{idx}\",  # Ajustar el nombre del archivo si se desea\n",
    "            'vis': vis\n",
    "        }\n",
    "\n",
    "        parameters_list.append(parameters)\n",
    "\n",
    "    return parameters_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
